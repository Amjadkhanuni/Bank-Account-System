{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11198bb1-964a-4263-b271-61fa00255599",
   "metadata": {},
   "source": [
    "# AMJAD KHAN   BSCS  USTB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fca52d",
   "metadata": {},
   "source": [
    "# Email spam Detection with Machine Learning  and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95899d-2d36-4661-aba0-744c04125fda",
   "metadata": {},
   "source": [
    "## 1. import numpy as np\n",
    "Purpose: Imports the NumPy library and gives it the alias np.\n",
    "\n",
    "NumPy: A powerful library for numerical computations in Python. It is widely used for handling arrays, performing mathematical operations, and working with multi-dimensional data.\n",
    "\n",
    "Example Use: np.array([1, 2, 3]) creates a NumPy array.\n",
    "## 2. import pandas as pd\n",
    "Purpose: Imports the Pandas library and gives it the alias pd.\n",
    "\n",
    "Pandas: A library for data manipulation and analysis. It provides two main data structures:\n",
    "\n",
    "DataFrame: Tabular data similar to Excel spreadsheets.\n",
    "\n",
    "Series: One-dimensional labeled data.\n",
    "\n",
    "Example Use: pd.DataFrame({'A': [1, 2, 3]}) creates a simple DataFrame.\n",
    "## 3. import nltk\n",
    "Purpose: Imports the NLTK (Natural Language Toolkit) library.\n",
    "\n",
    "NLTK: A library for natural language processing tasks like tokenization, stemming, lemmatization, and more.\n",
    "\n",
    "Example Use: nltk.word_tokenize(\"Hello, world!\") splits the sentence into words.\n",
    "## 4. from nltk.corpus import stopwords\n",
    "Purpose: Imports the stopwords module from the nltk.corpus.\n",
    "\n",
    "Stopwords: Common words like \"the\", \"is\", \"and\" that are often removed in text processing because they do not contribute much meaning.\n",
    "\n",
    "Example Use: stopwords.words('english') provides a list of English stopwords.\n",
    "## 5. import string\n",
    "Purpose: Imports Pythonâ€™s built-in string module.\n",
    "\n",
    "String Module: Provides useful constants and classes for string manipulation.\n",
    "\n",
    "string.punctuation: A string of all punctuation characters (!\"#$%&'()*+,-./:;<=>?@[\\\\]^_{|}~`).\n",
    "\n",
    "string.ascii_letters: A string of all ASCII letters (abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ).\n",
    "\n",
    "Example Use: string.punctuation is often used to remove punctuation in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee00e4",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb4f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd   \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c05ea",
   "metadata": {},
   "source": [
    "## Load the data and print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50086a92-3b59-4564-9dc7-6566467aadab",
   "metadata": {},
   "source": [
    "## 1. df = pd.read_csv(\"emails.csv\")\n",
    "Purpose: Reads a CSV (Comma-Separated Values) file named emails.csv into a Pandas DataFrame.\n",
    "Components:\n",
    "\n",
    "pd.read_csv: A Pandas function that loads data from a CSV file.\n",
    "\n",
    "\"emails.csv\": The file to be read. It must be in the same directory as your script or provide the full file path.\n",
    "\n",
    "## 2. df.head()\n",
    "Purpose: Displays the first five rows of the DataFrame df by default.\n",
    "\n",
    "Components:\n",
    "\n",
    ".head(): A Pandas method that allows you to preview the top rows of the DataFrame. You can specify the number of rows to display,\n",
    "\n",
    "e.g., df.head(5) for 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53483506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"emails.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439649ca-69d3-43af-a53a-d6ad7071b57c",
   "metadata": {},
   "source": [
    "## df.shape\n",
    "Purpose: Returns the dimensions of the DataFrame df as a tuple.\n",
    "\n",
    "Output: A tuple of two values:\n",
    "\n",
    "Number of rows (records).\n",
    "\n",
    "Number of columns (features or attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b34656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffceb41-6b79-4487-b437-7175719ac7f0",
   "metadata": {},
   "source": [
    "# df.columns\n",
    "## Explanation:\n",
    "Purpose: Returns the column names of the DataFrame df as a Pandas Index object.\n",
    "Output: A list-like object containing all column names in the same order as they appear in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c255c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'spam'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fa52a",
   "metadata": {},
   "source": [
    "# Check for duplicates and remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb98280-dcb6-4e69-91d0-f06f65d8b9da",
   "metadata": {},
   "source": [
    "## 1. df.drop_duplicates(inplace=True)\n",
    "## Explanation:\n",
    "Purpose: Removes duplicate rows from the DataFrame df.\n",
    "\n",
    "### Components:\n",
    "df.drop_duplicates(): A Pandas method to drop rows that have identical values across all columns (i.e., duplicates).\n",
    "\n",
    "inplace=True: Modifies the original DataFrame directly instead of creating a new DataFrame.\n",
    "\n",
    "## 2. print(df.shape)\n",
    "### Explanation:\n",
    "Purpose: Prints the dimensions of the DataFrame df after removing duplicates.\n",
    "### Components:\n",
    "df.shape: Returns the tuple (number_of_rows, number_of_columns).\n",
    "\n",
    "print(): Outputs this tuple to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5830a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5695, 2)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68931118",
   "metadata": {},
   "source": [
    "## See the number of missing data for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b187784-1e37-4557-a08a-742977ecd9c1",
   "metadata": {},
   "source": [
    "# print(df.isnull().sum())\n",
    "## Explanation:\n",
    "This line of code checks for missing (null) values in the DataFrame df and prints the count of null values for each column.\n",
    "### df.isnull():\n",
    "\n",
    "Purpose: Creates a new DataFrame of the same shape as df where each cell contains:\n",
    "\n",
    "True if the value in that cell is null (i.e., missing or NaN).\n",
    "\n",
    "False otherwise.\n",
    "\n",
    "### .sum():\n",
    "\n",
    "Purpose: Sums the True values column-wise (treating True as 1 and False as 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "989ae8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    0\n",
      "spam    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8395e34",
   "metadata": {},
   "source": [
    "## Download the stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54803388-872c-4650-ba6d-acdc8eea35b4",
   "metadata": {},
   "source": [
    "3 nltk.download(\"stopwords\")\n",
    "* Purpose: Downloads the stopwords dataset from the NLTK library.\n",
    "\n",
    "* What are Stopwords? Common words like \"the\", \"is\", \"and\", etc., that are often removed in text processing because they don't add much meaning.\n",
    "\n",
    "* Why Download? Stopwords are not included by default, so they need to be downloaded before use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c1d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amjad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the stopwords package\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ffada-6185-4ec4-8d50-f7e03565bb65",
   "metadata": {},
   "source": [
    "# def process(text):\n",
    "Purpose: Defines a function named process that takes a single argument, text.\n",
    "\n",
    "# nopunc = [char for char in text if char not in string.punctuation]\n",
    "Purpose: Creates a list of characters from text excluding punctuation.\n",
    "* How:\n",
    "Loops through each character (char) in text.\n",
    "\n",
    "Keeps the character only if it is not in string.punctuation (a predefined string of common punctuation marks like .,!?\").\n",
    "\n",
    "#### Result: A list of characters without punctuation.\n",
    "\n",
    "# nopunc = ''.join(nopunc)\n",
    "Purpose: Converts the list of characters back into a single string.\n",
    "\n",
    "How: Joins all characters in the nopunc list.\n",
    "\n",
    "Result: A string with punctuation removed.\n",
    "\n",
    "# clean = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "Purpose: Removes common stopwords (like \"the\", \"is\") from the text.\n",
    "How:\n",
    "nopunc.split(): Splits the string into a list of words.\n",
    "\n",
    "Loops through each word in this list.\n",
    "\n",
    "Keeps the word only if it (converted to lowercase) is not in the list of English stopwords (stopwords.words('english')).\n",
    "\n",
    "Result: A list of meaningful words without stopwords.\n",
    "\n",
    "# return clean\n",
    "Purpose: Outputs the list of cleaned words from the function.\n",
    "\n",
    "Example Output: ['hello', 'world', 'test']\n",
    "\n",
    "# df['text'].head().apply(process)\n",
    "Purpose: Applies the process function to the first 5 rows of the text column in the DataFrame df.\n",
    "## Steps:\n",
    "df['text']: Selects the text column from df.\n",
    "\n",
    ".head(): Takes the first 5 rows of this column.\n",
    "\n",
    ".apply(process): Applies the process function to each row (i.e., to each text entry).\n",
    "\n",
    "Result: Returns a Pandas Series where each entry is a list of cleaned words for the corresponding text in the first 5 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7cd220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Subject, naturally, irresistible, corporate, ...\n",
       "1    [Subject, stock, trading, gunslinger, fanny, m...\n",
       "2    [Subject, unbelievable, new, homes, made, easy...\n",
       "3    [Subject, 4, color, printing, special, request...\n",
       "4    [Subject, money, get, software, cds, software,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "\n",
    "    clean = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean\n",
    "# to show the tokenization\n",
    "df['text'].head().apply(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf9945",
   "metadata": {},
   "source": [
    "# convert the text into a matrix of token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e79ce1-a012-4f72-a08d-3eed749e3c79",
   "metadata": {},
   "source": [
    "## 1. Importing the CountVectorizer\n",
    "### from sklearn.feature_extraction.text import CountVectorizer\n",
    "Purpose: Imports the CountVectorizer class from the sklearn.feature_extraction.text module.\n",
    "\n",
    "#### What is CountVectorizer?\n",
    "It converts a collection of text documents into a matrix of token counts (a Bag of Words model).\n",
    "Each column in the matrix represents a unique word (token), and the cell value represents the word's count in a specific document.\n",
    "\n",
    "## 2. Using CountVectorizer with a Custom Analyzer\n",
    "### message = CountVectorizer(analyzer=process).fit_transform(df['text'])\n",
    "\n",
    "#### CountVectorizer(analyzer=process)\n",
    "Purpose: Initializes the CountVectorizer object with a custom analyzer.\n",
    "\n",
    "Analyzer:Defines how the text will be preprocessed and tokenized before converting into a Bag of Words.\n",
    "\n",
    "Here, process (the custom function defined earlier) is passed as the analyzer.\n",
    "\n",
    "process ensures:Text is cleaned (removes punctuation and stopwords).\n",
    "\n",
    "Words are tokenized into meaningful terms.\n",
    "\n",
    "#### .fit_transform(df['text'])\n",
    "\n",
    "Purpose: Learns the vocabulary (unique tokens) from the text column of df and transforms the text data into a sparse matrix representation.\n",
    "###### Components:\n",
    "fit: Extracts all unique words (tokens) across the text column and builds a vocabulary.\n",
    "\n",
    "transform: Converts each document into a row of token counts based on the learned vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32e88682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "message = CountVectorizer(analyzer=process).fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cdd6c-3910-498e-80c8-6e157d8f0095",
   "metadata": {},
   "source": [
    "## from sklearn.model_selection import train_test_split\n",
    "Imports the train_test_split function to split the data into training and testing sets.\n",
    "\n",
    "## xtrain, xtest, ytrain, ytest = train_test_split(message, df['spam'], test_size=0.20, random_state=0)\n",
    "Purpose: Splits the data into training and testing sets.\n",
    "Inputs: message: Features (Bag of Words matrix).\n",
    "\n",
    "df['spam']: Labels (target column indicating spam or not).\n",
    "\n",
    "test_size=0.20: 20% of the data is reserved for testing.\n",
    "\n",
    "random_state=0: Ensures reproducibility of the split.\n",
    "\n",
    "Ensures reproducibility means -----> The ability to obtain consistent results\n",
    "\n",
    "Outputs:\n",
    "xtrain: Features for training (80%).\n",
    "\n",
    "xtest: Features for testing (20%).\n",
    "\n",
    "ytrain: Labels for training (80%).\n",
    "\n",
    "ytest: Labels for testing (20%).\n",
    "\n",
    "### print(message.shape)\n",
    "Prints the shape (dimensions) of the message sparse matrix.\n",
    "\n",
    "Output: (n_rows, n_columns)\n",
    "\n",
    "n_rows: Number of documents (messages).\n",
    "\n",
    "n_columns: Number of unique words (vocabulary size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d4c2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5695, 37229)\n"
     ]
    }
   ],
   "source": [
    "#split the data into 80% training and 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(message, df['spam'], test_size=0.20, random_state=0)\n",
    "# To see the shape of the data\n",
    "print(message.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919af33-858c-4b6d-8e88-00806a1c6880",
   "metadata": {},
   "source": [
    "## from sklearn.naive_bayes import MultinomialNB\n",
    "### Imports the MultinomialNB class from sklearn.naive_bayes.\n",
    "Purpose: Multinomial Naive Bayes is a probabilistic classifier commonly used for text classification, especially for Bag of Words data.\n",
    "### classifier = MultinomialNB()           \n",
    "Creates an instance of the MultinomialNB classifier.\n",
    "\n",
    "This classifier assumes: Features are counts or frequencies (like in Bag of Words or TF-IDF representations).\n",
    "\n",
    "Follows the Naive Bayes theorem to calculate probabilities.\n",
    "### .fit(xtrain, ytrain)\n",
    "Trains the Naive Bayes classifier on the training data.\n",
    "##### Inputs:\n",
    "xtrain: Sparse matrix of training features (word counts for each document).\n",
    "\n",
    "ytrain: Labels (spam or not) for the corresponding documents.\n",
    "##### Output: The classifier is trained and ready to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e13bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train the Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f8fdf",
   "metadata": {},
   "source": [
    "# See the classifiers prediction and actual values on the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2a107-9516-4116-a310-5d76c87b2043",
   "metadata": {},
   "source": [
    "## 1. print(classifier.predict(xtrain))\n",
    "Purpose: Predicts the labels (spam or not) for the training data xtrain.\n",
    "\n",
    "Steps:\n",
    "\n",
    "classifier.predict(xtrain):  Uses the trained Naive Bayes model (classifier) to classify each document in xtrain (training features).\n",
    "### Outputs an array of predicted labels (e.g., [0, 1, 0, 1, ...]), where:\n",
    "0 means \"not spam.\"\n",
    "\n",
    "1 means \"spam.\"\n",
    "###### Output: Prints the array of predicted labels for the training data.\n",
    "\n",
    "## 2. print(ytrain.values)\n",
    "Purpose: Prints the true labels (actual spam or not) for the training data.\n",
    "Steps: \n",
    "ytrain.values:   Converts the ytrain Series (Pandas column) to a NumPy array.\n",
    "\n",
    "This contains the actual labels (e.g., [0, 1, 0, 1, ...]) corresponding to xtrain.\n",
    "###### Output: Prints the true labels for comparison with the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25f36d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(xtrain))\n",
    "print(ytrain.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa86ace-078c-4c07-a89a-15f24ad8124c",
   "metadata": {},
   "source": [
    "## from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "Purpose: Imports evaluation functions from sklearn.metrics to assess the model's performance.\n",
    "\n",
    "classification_report: Provides detailed metrics like precision, recall, F1-score, and support for each class.\n",
    "\n",
    "confusion_matrix: Computes the confusion matrix, which shows the count of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "accuracy_score: Calculates the accuracy of the model, i.e., the percentage of correct predictions.\n",
    "\n",
    "## pred = classifier.predict(xtrain)\n",
    "Purpose: Makes predictions on the training set (xtrain) using the trained Naive Bayes classifier (classifier).\n",
    "#### Result:\n",
    "pred will be an array of predicted labels (e.g., [0, 1, 0, 1, ...]), where 0 is \"not spam\" and 1 is \"spam\".\n",
    "\n",
    "## print(classification_report(ytrain, pred))\n",
    "Purpose: Prints the classification report comparing the true labels (ytrain) with the predicted labels (pred).\n",
    "\n",
    "##### Explanation:\n",
    "Precision: The percentage of correct positive predictions (spam) out of all predicted positives.\n",
    "\n",
    "Recall: The percentage of correct positive predictions (spam) out of all actual positives (spam).\n",
    "\n",
    "F1-score: The harmonic mean of precision and recall.\n",
    "\n",
    "Support: The number of occurrences of each class in the dataset.\n",
    "\n",
    "## print()\n",
    "Purpose: Prints a blank line for better readability between the outputs.\n",
    "# print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "Purpose: Prints the confusion matrix that shows the breakdown of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "Explanation: The confusion matrix for binary classification is a 2x2 matrix.\n",
    "\n",
    "TN: True Negative (not spam correctly predicted as not spam)\n",
    "\n",
    "FP: False Positive (not spam incorrectly predicted as spam)\n",
    "\n",
    "FN: False Negative (spam incorrectly predicted as not spam)\n",
    "\n",
    "TP: True Positive (spam correctly predicted as spam)\n",
    "\n",
    "# print(\"Accuracy: \\n\", accuracy_score(ytrain, pred))\n",
    "\n",
    "Purpose: Prints the accuracy of the model, which is the ratio of correct predictions to the total number of predictions.\n",
    "\n",
    "## Formula:\n",
    "Accuracy  =   NumberÂ ofÂ CorrectÂ Predictions  /  TotalÂ NumberÂ ofÂ Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a86c9d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3457\n",
      "           1       0.99      1.00      0.99      1099\n",
      "\n",
      "    accuracy                           1.00      4556\n",
      "   macro avg       0.99      1.00      1.00      4556\n",
      "weighted avg       1.00      1.00      1.00      4556\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3445   12]\n",
      " [   1 1098]]\n",
      "Accuracy: \n",
      " 0.9971466198419666\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training data set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "pred = classifier.predict(xtrain)\n",
    "print(classification_report(ytrain, pred))\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "print(\"Accuracy: \\n\", accuracy_score(ytrain, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe807a6-a910-48e1-87c0-ac88a2a022cd",
   "metadata": {},
   "source": [
    "## 1. print(classifier.predict(xtest))\n",
    "Purpose: Prints the predicted labels for the test set (xtest) using the trained Naive Bayes classifier (classifier).\n",
    "##### Steps:\n",
    "classifier.predict(xtest):   Passes the feature matrix of the test set (xtest) to the classifier's predict method.\n",
    "\n",
    "Outputs an array of predictions for each document in the test set.\n",
    "\n",
    "Predicted Labels: Typically 0 (not spam) or 1 (spam)\n",
    "\n",
    "## 2. print(ytest.values)\n",
    "Purpose: Prints the actual labels (ground truth) for the test set (ytest).\n",
    "\n",
    "##### Steps:\n",
    "ytest.values:  Converts the ytest Series (Pandas column) into a NumPy array.\n",
    "\n",
    "Contains the true labels (e.g., [0, 1, 0, 0, 1]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e125bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 0]\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print the predictions\n",
    "print(classifier.predict(xtest))\n",
    "#print the actual values\n",
    "print(ytest.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbc1f4",
   "metadata": {},
   "source": [
    "# evaluate the model on the test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb84383-37e3-4a26-bf94-2ddba2434243",
   "metadata": {},
   "source": [
    "## from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "Purpose: Imports evaluation metrics from sklearn.metrics to assess the model's performance on the test data.\n",
    "\n",
    "classification_report:   Summarizes key metrics like precision, recall, F1-score, and support for each class.\n",
    "\n",
    "confusion_matrix:   Provides a breakdown of correct and incorrect predictions.\n",
    "\n",
    "accuracy_score:   Calculates the proportion of correct predictions.\n",
    "\n",
    "# pred = classifier.predict(xtest)\n",
    "Purpose: Makes predictions on the test dataset (xtest) using the trained Naive Bayes classifier (classifier).\n",
    "##### Output:\n",
    "pred is an array containing the predicted labels (e.g., [0, 1, 0, 1, ...]) for each document in the test set.\n",
    "\n",
    "These predictions will be compared with the actual labels (ytest) to evaluate the model.\n",
    "\n",
    "## print(classification_report(ytest, pred))\n",
    "Purpose:   Prints the classification report comparing the true labels (ytest) with the predicted labels (pred).\n",
    "#### Explanation of Metrics:\n",
    "Precision:   The fraction of correctly identified positive predictions (spam) out of all predicted positives.\n",
    "\n",
    "Recall:    The fraction of correctly identified positive predictions (spam) out of all actual positives (spam).\n",
    "\n",
    "F1-score:   The harmonic mean of precision and recall, providing a balance between them.\n",
    "\n",
    "Support:   The number of true instances for each class in the dataset.\n",
    "\n",
    "## print(\"Accuracy: \\n\", accuracy_score(ytest, pred))\n",
    "\n",
    "Purpose: Prints the overall accuracy of the model on the test data.\n",
    "\n",
    "### Formula:\n",
    "Accuracy = NumberÂ ofÂ CorrectÂ Predictions  /  TotalÂ NumberÂ ofÂ Predictions\n",
    "\n",
    "â€‹\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6b1decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       870\n",
      "           1       0.97      1.00      0.98       269\n",
      "\n",
      "    accuracy                           0.99      1139\n",
      "   macro avg       0.98      0.99      0.99      1139\n",
      "weighted avg       0.99      0.99      0.99      1139\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[862   8]\n",
      " [  1 268]]\n",
      "Accuracy: \n",
      " 0.9920983318700615\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training data set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "pred = classifier.predict(xtest)\n",
    "print(classification_report(ytest, pred))\n",
    "print()\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred))\n",
    "print(\"Accuracy: \\n\", accuracy_score(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcee95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
